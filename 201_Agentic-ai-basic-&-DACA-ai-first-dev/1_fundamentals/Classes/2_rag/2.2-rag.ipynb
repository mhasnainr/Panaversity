{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Project 2: LangChain RAG Project\n","## This Chatboat Have All Information But Only About\n","  * Artificial Intelligence  \n","  * Agentic AI\n","  * Means All About AI"],"metadata":{"id":"ZVLAxaZhCXxB"}},{"cell_type":"markdown","source":["# RAG-based QA System using LangChain, Pinecone, and Google Generative AI\n","\n","This project notebook demonstrates the creation of a **Retrieval-Augmented Generation (RAG)**-based Question Answering (QA) system. It leverages powerful tools and APIs such as **LangChain**, **Pinecone**, and **Google Generative AI (Gemini Model)** to provide contextual answers to user queries based on uploaded documents. The notebook showcases a step-by-step process, from loading and splitting documents to creating embeddings, uploading data to Pinecone, and querying the system for responses.\n","\n","## Key Features\n","1. **Pinecone for Vector Storage**  \n","   The project uses Pinecone as the vector database to store and retrieve document embeddings efficiently.  \n","   \n","2. **LangChain Integration**  \n","   LangChain simplifies document processing by handling text splitting, embedding generation, and connecting various AI components.  \n","   \n","3. **Google Generative AI (Gemini)**  \n","   Utilizes the Gemini 1.5 model for both embedding generation and language model-based reasoning to deliver intelligent and context-aware responses.  \n","\n","4. **RAG Approach**  \n","   Combines retrieval and generation by searching for the most relevant document chunks and using an LLM to formulate coherent answers.  \n","\n","5. **Flexible Querying**  \n","   Users can input any query, and the system retrieves the most relevant information from the indexed documents to provide accurate answers.  \n","\n","## Workflow\n","1. **Setup**  \n","   Install and configure the required libraries such as LangChain, Pinecone Client, Google Generative AI, and TQDM.  \n","\n","2. **Embedding Generation**  \n","   Process documents into chunks and generate embeddings using Google Generative AI's embedding model.  \n","\n","3. **Vector Indexing**  \n","   Store the generated embeddings in a Pinecone index for efficient similarity-based retrieval.  \n","\n","4. **Query Handling**  \n","   User queries are processed through a retrieval-augmented chain that fetches the most relevant documents, formulates a response, and displays the result.\n","\n","5. **Source Document Traceability**  \n","   Returns the source documents used in formulating each response for transparency and validation.\n","\n","## Use Cases\n","- Building document-based question-answering systems.\n","- Enhancing productivity by allowing quick access to relevant information from large datasets.\n","- Supporting tasks in education, research, and knowledge management.\n","\n","## Technologies Used\n","- **LangChain**: Framework for building AI-powered applications.\n","- **Pinecone**: Vector database for storing and retrieving embeddings.\n","- **Google Generative AI (Gemini Model)**: Advanced generative AI for embedding and LLM capabilities.\n","- **Python**: Programming language for integrating APIs and data processing.\n","\n","This project highlights the potential of combining vector search with generative AI to enable sophisticated knowledge retrieval systems.\n"],"metadata":{"id":"AkoHY-em34m2"}},{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"id":"u9Fx-pmQCLmh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749898432297,"user_tz":-300,"elapsed":15001,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}},"outputId":"868d6d9a-2f32-48b2-ff4b-400ccd118097"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -qU langchain langchain-pinecone tqdm"]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n","os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n","\n","GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n","os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n","PINECONE_ENVIRONMENT = 'us-east-1'"],"metadata":{"id":"Rc6yaOIgGbQZ","executionInfo":{"status":"ok","timestamp":1749898436598,"user_tz":-300,"elapsed":2168,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from pinecone import Pinecone, ServerlessSpec\n","\n","\n","pc = Pinecone(\n","    api_key=PINECONE_API_KEY\n",")\n","\n","# Check if the index exists; if not, create it\n","index_name = \"anewindex\"\n","if index_name not in pc.list_indexes().names():\n","    pc.create_index(\n","        name=index_name,\n","        dimension=768,\n","        metric=\"cosine\",  # Choose the metric: cosine, euclidean, or dotproduct\n","        spec=ServerlessSpec(\n","            cloud=\"aws\",\n","            region=PINECONE_ENVIRONMENT  # Use your environment's region\n","        )\n","    )\n","\n","# # Connect to the index\n","index = pc.Index(name=index_name)\n","print(f\"Successfully connected to index: {index_name}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muJ6Mz0DHb35","outputId":"353c9053-eb70-4ba9-fba9-02e4a8e7f750","executionInfo":{"status":"ok","timestamp":1749898439625,"user_tz":-300,"elapsed":968,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully connected to index: anewindex\n"]}]},{"cell_type":"code","source":["!pip install -q -U langchain-google-genai"],"metadata":{"id":"BnWIARJrLK75","collapsed":true,"executionInfo":{"status":"ok","timestamp":1749898468918,"user_tz":-300,"elapsed":12273,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n","\n","# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY # Assuming PINECONE_API_KEY is already defined\n","\n","\n","embeddings = GoogleGenerativeAIEmbeddings(\n","    model=\"models/embedding-001\",  # Specify the desired embedding model\n","    api_key=GOOGLE_API_KEY\n",")"],"metadata":{"id":"dXGcUcqeKcah","executionInfo":{"status":"ok","timestamp":1749898479025,"user_tz":-300,"elapsed":5347,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["!pip install -q -U langchain-community"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aoDT7AgaLn_x","outputId":"2637ab39-375d-4363-bdb4-a1e111cb6fba","collapsed":true,"executionInfo":{"status":"ok","timestamp":1749898493348,"user_tz":-300,"elapsed":12676,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["from langchain.document_loaders import TextLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# Load documents\n","loader = TextLoader(\"/content/document.txt\")  # Replace with your file\n","documents = loader.load()\n","\n","# Split documents into chunks\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n","docs = text_splitter.split_documents(documents)"],"metadata":{"id":"NH-wCojMLd8M","executionInfo":{"status":"ok","timestamp":1749898495753,"user_tz":-300,"elapsed":71,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["\n","from tqdm import tqdm\n","\n","# Create embeddings and upload to Pinecone\n","for doc in tqdm(docs):\n","    vector = embeddings.embed_query(doc.page_content)\n","\n","    # Modify the upsert to use a dictionary for metadata\n","    index.upsert([{\n","        \"id\": doc.metadata[\"source\"],  # Use \"id\" instead of the first tuple element\n","        \"values\": vector,  # Use \"values\" for the vector\n","        \"metadata\": {\n","            \"text\": doc.page_content,  # Add the text as part of metadata\n","            \"source\": doc.metadata[\"source\"]  # Include the source\n","        }\n","    }])\n"],"metadata":{"id":"D2VCw_OfFbzL","outputId":"bdc7de7a-04c1-4de6-f7ad-814644ddf255","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749898503856,"user_tz":-300,"elapsed":5836,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 21/21 [00:05<00:00,  3.69it/s]\n"]}]},{"cell_type":"code","source":["from langchain_pinecone import PineconeVectorStore\n","\n","# Use from_existing_index to load from an existing index\n","# The from_existing_index method in PineconeVectorStore takes the index_name and embedding directly\n","retriever = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embeddings)"],"metadata":{"id":"-09FA0Opzqn5","executionInfo":{"status":"ok","timestamp":1749898511813,"user_tz":-300,"elapsed":2723,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.runnables import RunnablePassthrough,RunnableMap\n","\n","message = \"\"\"\n","Answer this question using the provided context only.\n","\n","{question}\n","\n","Context:\n","{context}\n","\"\"\""],"metadata":{"id":"D45Qz23sfLI2","executionInfo":{"status":"ok","timestamp":1749898519663,"user_tz":-300,"elapsed":123,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["prompt = ChatPromptTemplate.from_messages([(\"human\", message)])"],"metadata":{"id":"CdGvFWNb9och","executionInfo":{"status":"ok","timestamp":1749898523526,"user_tz":-300,"elapsed":190,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","gemini_model = ChatGoogleGenerativeAI(api_key=GOOGLE_API_KEY,model=\"gemini-1.5-flash\", temperature=0.7)"],"metadata":{"id":"-8OUqrpu0Gdf","executionInfo":{"status":"ok","timestamp":1749898529196,"user_tz":-300,"elapsed":32,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["rag_chain = RunnableMap({\n","    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"])[0], # Get the first relevant document\n","    \"question\": RunnablePassthrough()\n","}) | prompt | gemini_model\n"],"metadata":{"id":"h5hdO9Fr9sPx","executionInfo":{"status":"ok","timestamp":1749898533608,"user_tz":-300,"elapsed":256,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from langchain_pinecone import PineconeVectorStore\n","from langchain.chains import RetrievalQA\n","\n","# Create a vector store using the Pinecone index\n","vectorstore = PineconeVectorStore(\n","    index_name=index_name,\n","    embedding=embeddings\n",")\n","\n","# Create the retriever\n","retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})  # Retrieve top 2 most similar documents\n","\n","# Create the QA chain\n","qa_chain = RetrievalQA.from_chain_type(\n","    llm=gemini_model,\n","    chain_type=\"stuff\",\n","    retriever=retriever,  # Pass the retriever here\n","    return_source_documents=True  # Optional: to get the source documents used in the response\n",")"],"metadata":{"id":"pEMQYqQk1Dgt","executionInfo":{"status":"ok","timestamp":1749898540955,"user_tz":-300,"elapsed":442,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["vectorstore.similarity_search(\"Future?\", k=3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RIZbA7CDsOL0","outputId":"e755c89d-03f9-45fd-d0cf-3fbfe16dd7ef","executionInfo":{"status":"ok","timestamp":1749898953669,"user_tz":-300,"elapsed":671,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(id='/content/skills.txt', metadata={'source': '/content/skills.txt'}, page_content='By consistently following these practices and observing these tech leaders, you’ll start building these skills. Since your goal is to create something valuable, the time invested now will create a foundation that will serve you well in your career.'),\n"," Document(id='/content/document.txt', metadata={'source': '/content/document.txt'}, page_content='By consistently following these practices and observing these tech leaders, you’ll start building these skills. Since your goal is to create something valuable, the time invested now will create a foundation that will serve you well in your career.'),\n"," Document(id='/content/stuff.txt', metadata={'source': '/content/stuff.txt'}, page_content='By consistently following these practices and observing these tech leaders, you’ll start building these skills. Since your goal is to create something valuable, the time invested now will create a foundation that will serve you well in your career.')]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["print(\"Welcome To RAG Project\")\n","print('='*40)\n","\n","query = \"What is there in the file?\"\n","print('Human Message:',query)\n","response = qa_chain.invoke(query)\n","\n","# Print the answer\n","print(\"Agent Message:\", response['result'])\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_3z1a9t1YZc","outputId":"de02d8b2-0eff-4dc1-c091-e2efc77dafa0","executionInfo":{"status":"ok","timestamp":1749898984826,"user_tz":-300,"elapsed":1062,"user":{"displayName":"Hasnain","userId":"07027081452957922943"}}},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Welcome To RAG Project\n","========================================\n","Human Message: What is inside the document?\n","Agent Message: The provided text is a short excerpt offering advice on skill-building by observing tech leaders and consistently following best practices.  It emphasizes the long-term value of this investment.  It doesn't describe the contents of a specific document.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"OdE8OJVxFODe"},"execution_count":null,"outputs":[]}]}